{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from mnist import MNIST\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "def one_hot_encode(np_array):\n",
    "    return (np.arange(10) == np_array[:,None]).astype(np.float32)\n",
    "\n",
    "def reformat_data(dataset, labels, image_width, image_height, image_depth):\n",
    "    np_dataset_ = np.array([np.array(image_data).reshape(image_width, image_height, image_depth) for image_data in dataset])\n",
    "    np_labels_ = one_hot_encode(np.array(labels, dtype=np.float32))\n",
    "    np_dataset, np_labels = randomize(np_dataset_, np_labels_)\n",
    "    return np_dataset, np_labels\n",
    "\n",
    "def flatten_array(array):\n",
    "    shape = array.shape\n",
    "    return array.reshape([shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "def flatten_tf_array (layer):\n",
    "    num_features = layer.get_shape()[1:4].num_elements()\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_folder = './data/mnist/'\n",
    "mnist_image_width = 28\n",
    "mnist_image_height = 28\n",
    "mnist_image_depth = 1\n",
    "mnist_num_labels = 10\n",
    "\n",
    "mndata = MNIST(mnist_folder)\n",
    "# mnist_train_dataset_: list of len 60000; eachelement list of 784 len; mnist_train_labels_: list of len 60000; eachelement one int\n",
    "mnist_train_dataset_, mnist_train_labels_ = mndata.load_training()\n",
    "# mnist_test_dataset_: list of len 10000; eachelement list of 784 len; mnist_test_labels_: list of len 10000; eachelement one int\n",
    "mnist_test_dataset_, mnist_test_labels_ = mndata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist_train_dataset: (60000, 28, 28, 1); mnist_train_labels: (60000, 10)\n",
    "mnist_train_dataset, mnist_train_labels = reformat_data(mnist_train_dataset_, mnist_train_labels_, mnist_image_width, mnist_image_height, mnist_image_depth)\n",
    "# mnist_test_dataset: (10000, 28, 28, 1); mnist_test_labels: (10000, 10)\n",
    "mnist_test_dataset, mnist_test_labels = reformat_data(mnist_test_dataset_, mnist_test_labels_, mnist_image_width, mnist_image_height, mnist_image_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 64\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defien the placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_x = [None] + list(mnist_train_dataset.shape)[1:4] # batch_size, image_width, image_height, image_depth\n",
    "shape_y = [None] + list(mnist_train_labels.shape)[1:4] # batch_size, num_classes\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=shape_x)\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=shape_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for initializing new variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for creating a new convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv_layer(input, num_input_channels, filter_size, num_filters, use_pooling):\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "    weights = new_weights(shape=shape)\n",
    "    biases = new_biases(length=num_filters)\n",
    "    layer = tf.nn.conv2d(input=input, filter=weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    layer += biases\n",
    "    if use_pooling:\n",
    "        layer = tf.nn.max_pool(value=layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for creating a new fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_fc_layer(input, num_inputs, num_outputs, use_relu, use_sigmoid):\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs]) # number of outputs= num_neurons\n",
    "    biases = new_biases(length=num_outputs)\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    if use_sigmoid:\n",
    "        layer = tf.nn.sigmoid(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model (graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_layers = mnist_train_dataset.shape[-1]\n",
    "filter_size_c1 = 5\n",
    "filter_size_c2 = 5\n",
    "num_filters_c1 = 16\n",
    "num_filters_c2 = 36\n",
    "n_hidden_1 = 120\n",
    "n_hidden_2 = 80\n",
    "num_classes = mnist_train_labels.shape[1]\n",
    "\n",
    "# 1st convolutional layer\n",
    "layer_conv1, weights_conv1 = new_conv_layer(input=X, num_input_channels=img_layers, filter_size=filter_size_c1, num_filters=num_filters_c1, use_pooling=True)\n",
    "# 2nd convolutional layer\n",
    "layer_conv2, weights_conv2 = new_conv_layer(input=layer_conv1, num_input_channels=num_filters_c1, filter_size=filter_size_c2, num_filters=num_filters_c2, use_pooling=True)\n",
    "\n",
    "# 1st fully connected layer\n",
    "layer_flat, num_features = flatten_tf_array (layer_conv2) # flatten the last conv layer to input to the 1st fc layer\n",
    "layer_fc1 = new_fc_layer(input=layer_flat, num_inputs=num_features, num_outputs=n_hidden_1, use_relu=True, use_sigmoid=False)\n",
    "# 2nd fully connected layer\n",
    "layer_fc2 = new_fc_layer(input=layer_fc1, num_inputs=n_hidden_1, num_outputs=n_hidden_2, use_relu=True, use_sigmoid=False)\n",
    "#last fully connected layer (output layer)\n",
    "layer_fc3 = new_fc_layer(input=layer_fc2, num_inputs=n_hidden_2, num_outputs=num_classes, use_relu=False, use_sigmoid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = layer_fc3\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "y_pred = tf.nn.softmax(logits)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 89.456043, Test_acc: 96.000000\n",
      "Train acc: 96.177962, Test_acc: 97.320000\n",
      "Train acc: 97.088447, Test_acc: 97.650000\n",
      "Train acc: 97.573706, Test_acc: 97.980000\n",
      "Train acc: 97.903882, Test_acc: 98.090000\n",
      "Train acc: 98.075640, Test_acc: 98.220000\n",
      "Train acc: 98.272412, Test_acc: 98.190000\n",
      "Train acc: 98.374133, Test_acc: 98.250000\n",
      "Train acc: 98.489194, Test_acc: 98.390000\n",
      "Train acc: 98.562567, Test_acc: 98.440000\n",
      "Train acc: 98.615928, Test_acc: 98.430000\n",
      "Train acc: 98.702641, Test_acc: 98.500000\n",
      "Train acc: 98.730990, Test_acc: 98.670000\n",
      "Train acc: 98.821038, Test_acc: 98.620000\n",
      "Train acc: 98.836046, Test_acc: 98.540000\n",
      "Train acc: 98.894410, Test_acc: 98.820000\n",
      "Train acc: 98.959445, Test_acc: 98.750000\n",
      "Train acc: 99.001134, Test_acc: 98.640000\n",
      "Train acc: 99.021145, Test_acc: 98.720000\n",
      "Train acc: 99.067836, Test_acc: 98.640000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    total_batch_train = int(mnist_train_dataset.shape[0] / batch_size) # 60000/batch_size: number of batches for train dataset\n",
    "    \n",
    "    for iter_num in range(num_epochs):\n",
    "        avg_train_acc = 0.\n",
    "        for i in range(total_batch_train):\n",
    "            train_x = mnist_train_dataset[(i) * batch_size: (i + 1) * batch_size, ...] # (batch_size, 28, 28, 1)\n",
    "            train_y = mnist_train_labels[(i) * batch_size: (i + 1) * batch_size, ...] # (batch_size, 10)\n",
    "            feed_dict = {X: train_x, Y: train_y}\n",
    "            _, train_pred = sess.run([optimizer, y_pred], feed_dict=feed_dict) #train_pred:(batch_size, 10)\n",
    "            train_accuracy = accuracy(train_pred, train_y)\n",
    "            avg_train_acc += train_accuracy\n",
    "            \n",
    "        test_x = mnist_test_dataset # (10000, 28, 28, 1)\n",
    "        test_y = mnist_test_labels # (1000, 10)\n",
    "        feed_dict = {X: test_x, Y: test_y}\n",
    "        test_pred = sess.run(y_pred, feed_dict=feed_dict)\n",
    "        test_accuracy = accuracy(test_pred, test_y)\n",
    "        \n",
    "#         print the train and test accuracies\n",
    "        print(\"Train acc: %f, Test_acc: %f\" % (avg_train_acc/total_batch_train, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from mnist import MNIST\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "def one_hot_encode(np_array):\n",
    "    return (np.arange(10) == np_array[:,None]).astype(np.float32)\n",
    "\n",
    "def reformat_data(dataset, labels, image_width, image_height, image_depth):\n",
    "    np_dataset_ = np.array([np.array(image_data).reshape(image_width, image_height, image_depth) for image_data in dataset])\n",
    "    np_labels_ = one_hot_encode(np.array(labels, dtype=np.float32))\n",
    "    np_dataset, np_labels = randomize(np_dataset_, np_labels_)\n",
    "    return np_dataset, np_labels\n",
    "\n",
    "def flatten_tf_array(array):\n",
    "    shape = array.get_shape().as_list()\n",
    "    return tf.reshape(array, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_folder = './data/mnist/'\n",
    "mnist_image_width = 28\n",
    "mnist_image_height = 28\n",
    "mnist_image_depth = 1\n",
    "mnist_num_labels = 10\n",
    "\n",
    "mndata = MNIST(mnist_folder)\n",
    "\n",
    "# mnist_train_dataset_: list of len 60000; eachelement list of 784 len; mnist_train_labels_: list of len 60000; eachelement one int\n",
    "mnist_train_dataset_, mnist_train_labels_ = mndata.load_training()\n",
    "\n",
    "# mnist_test_dataset_: list of len 10000; eachelement list of 784 len; mnist_test_labels_: list of len 10000; eachelement one int\n",
    "mnist_test_dataset_, mnist_test_labels_ = mndata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist_train_dataset: (60000, 28, 28, 1); mnist_train_labels: (60000, 10)\n",
    "mnist_train_dataset, mnist_train_labels = reformat_data(mnist_train_dataset_, mnist_train_labels_, mnist_image_width, mnist_image_height, mnist_image_depth)\n",
    "\n",
    "# mnist_test_dataset: (10000, 28, 28, 1); mnist_test_labels: (10000, 10)\n",
    "mnist_test_dataset, mnist_test_labels = reformat_data(mnist_test_dataset_, mnist_test_labels_, mnist_image_width, mnist_image_height, mnist_image_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1 # ??????????????????????\n",
    "batch_size = 400 # 60000/400 = 150 batches\n",
    "num_epochs = 50\n",
    "n_input = mnist_image_width * mnist_image_height * mnist_image_depth # num of features: 784\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training inputs as placeholders, and weights and biases as variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_input]) # for each batch: (400, 4116)\n",
    "Y = tf.placeholder(\"float\", [None, n_classes]) # for each batch: (400, 10)\n",
    "\n",
    "# Weights and biases\n",
    "n_hidden_1 = 10  # number of neurons in hidden layers\n",
    "weights = {\n",
    "\t'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])), # 1st hidden layer: (784, 10)\n",
    "\t'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_1])), # 2nd hidden layer: (10, 10)\n",
    "\t'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes])) # output layer: (10, 10)\n",
    "}\n",
    "biases = {\n",
    "\t'b1': tf.Variable(tf.random_normal([n_hidden_1])), # bias for the 1st hidden layer: (10,)\n",
    "\t'b2': tf.Variable(tf.random_normal([n_hidden_1])), # bias for the 2nd hidden layer: (10,)\n",
    "\t'b_out': tf.Variable(tf.random_normal([n_classes])) # bias for the output layer: (10,)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['out1']), biases['out2']) # Added layer\n",
    "    out_layer = tf.nn.sigmoid(tf.matmul(layer_2, weights['out2']) + biases['out3'])\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = multilayer_perceptron(X) # >>>(?, 2)\n",
    "# Define loss(softmax_cross_entropy_with_logits) and optimizer(AdamOptimizer)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "\n",
    "    h = h5py.File('data1.hdf5', 'r')\n",
    "    n1 = h.get('data_train')\n",
    "    n2 = h.get('data_test')\n",
    "\n",
    "    # Training cycle\n",
    "    total_batch_train = int(n1.shape[0] / batch_size) # 60:number of batches for train dataset\n",
    "    print(total_batch_train)\n",
    "    total_batch_test = int(n2.shape[0] / batch_size) # 2:number of batches for test dataset\n",
    "    \n",
    "\n",
    "    for iter_num in range(num_epochs):\n",
    "        avg_acc_train = 0.\n",
    "        avg_acc_test = 0.\n",
    "        for i in range(total_batch_train):\n",
    "            train_x = n1[(i) * batch_size: (i + 1) * batch_size, ...]\n",
    "            train_y = labels_train[(i) * batch_size: (i + 1) * batch_size, :]\n",
    "\n",
    "            _, c_train, _logits_train = sess.run([train_op, loss_op, logits], feed_dict={X: train_x, Y: train_y})\n",
    "            _label_train = [np.argmax(i) for i in _logits_train]\n",
    "            _label_train_y = [np.argmax(i) for i in train_y]\n",
    "            _accuracy_train = np.mean(np.array(_label_train) == np.array(_label_train_y))\n",
    "            avg_acc_train += _accuracy_train\n",
    "\n",
    "\n",
    "        for j in range(total_batch_test):\n",
    "            test_x = n2[(j) * batch_size: (j + 1) * batch_size, ...]\n",
    "            test_y = labels_test[(j) * batch_size: (j + 1) * batch_size, :]\n",
    "\n",
    "            c_test, _logits_test = sess.run([loss_op, logits], feed_dict={X: test_x, Y: test_y})\n",
    "            _label_test = [np.argmax(i) for i in _logits_test]\n",
    "            _label_test_y = [np.argmax(i) for i in test_y]\n",
    "            _accuracy_test = np.mean(np.array(_label_test) == np.array(_label_test_y))\n",
    "            avg_acc_test += _accuracy_test\n",
    "\n",
    "        # print the train and test accuracies   \n",
    "        print(\"Train acc: %f, Test_acc: %f\" % (avg_acc_train/total_batch_train, avg_acc_test/total_batch_test))\n",
    "    duration = time.time() - start_time\n",
    "    print('Time elapsed - {} seconds.'.format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook MLP.ipynb to html\n",
      "[NbConvertApp] Writing 306490 bytes to ../html-output\\MLP.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --output-dir='../html-output' MLP.ipynb --to html --output MLP.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
